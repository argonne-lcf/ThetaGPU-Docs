{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Data Science Software Availability On Theta GPU, currently we support the major deep learning frameworks through two paths: singularity containers, based off of Nvidia's docker containers, and through bare-metal source builds. The bare-metal builds are so far only for tensorflow 2.X, with plans to support pytorch soon. Tensorflow 1.X is supported only via Nvidia's containers at this time. Containers As of now, the nvidia containers with tensorflow 1, 2 and pytorch built against cuda11, cudnn8 are available in singularity format here: $ ls /lus/theta-fs0/projects/datascience/thetaGPU/containers/ pytorch_20.08-py3.sif tf1_20.08-py3.sif tf2_20.08-py3.sif Execute a container interactively like this: $ singularity exec --nv -B /lus:/lus /lus/theta-fs0/projects/datascience/thetaGPU/containers/tf1_20.08-py3.sif bash","title":"Home"},{"location":"#data-science-software-availability","text":"On Theta GPU, currently we support the major deep learning frameworks through two paths: singularity containers, based off of Nvidia's docker containers, and through bare-metal source builds. The bare-metal builds are so far only for tensorflow 2.X, with plans to support pytorch soon. Tensorflow 1.X is supported only via Nvidia's containers at this time.","title":"Data Science Software Availability"},{"location":"#containers","text":"As of now, the nvidia containers with tensorflow 1, 2 and pytorch built against cuda11, cudnn8 are available in singularity format here: $ ls /lus/theta-fs0/projects/datascience/thetaGPU/containers/ pytorch_20.08-py3.sif tf1_20.08-py3.sif tf2_20.08-py3.sif Execute a container interactively like this: $ singularity exec --nv -B /lus:/lus /lus/theta-fs0/projects/datascience/thetaGPU/containers/tf1_20.08-py3.sif bash","title":"Containers"},{"location":"GPU%20Monitoring/","text":"GPU Monitoring Each GPU on ThetaGPU hosts 8 A100 GPUs. You can see information about these GPUs via the command nvidia-smi . Each GPU has 40Gb of on-GPU memory. When you run applications, you will know the GPU is in use when you see the memory increase and the GPU Utilization will be non-zero. You can target a specific GPU with nvidia-smi -i 0 for the first GPU, for example. GPU Selection In many application codes, you may want to specifiy which GPU is used. This is particular important in node-sharing applications where each GPU is running it's own code, which can be either in data-parallel model training, workflow based throughput jobs, etc. You can control individual process launches with: # Specify to run only on GPU 4: export CUDA_VISIBLE_DEVICES=4 # Let your application see GPUS 0, 1, and 7: export CUDA_VISIBLE_DEVICES=\"0,1,7\" In these cases, the GPU orderings will appear as a consecutive list starting with 0. From inside an application, many software frameworks have ability to let you target specific GPUs, including tensorflow and pytorch: Tensorflow Pytorch","title":"GPU Monitoring"},{"location":"GPU%20Monitoring/#gpu-monitoring","text":"Each GPU on ThetaGPU hosts 8 A100 GPUs. You can see information about these GPUs via the command nvidia-smi . Each GPU has 40Gb of on-GPU memory. When you run applications, you will know the GPU is in use when you see the memory increase and the GPU Utilization will be non-zero. You can target a specific GPU with nvidia-smi -i 0 for the first GPU, for example.","title":"GPU Monitoring"},{"location":"GPU%20Monitoring/#gpu-selection","text":"In many application codes, you may want to specifiy which GPU is used. This is particular important in node-sharing applications where each GPU is running it's own code, which can be either in data-parallel model training, workflow based throughput jobs, etc. You can control individual process launches with: # Specify to run only on GPU 4: export CUDA_VISIBLE_DEVICES=4 # Let your application see GPUS 0, 1, and 7: export CUDA_VISIBLE_DEVICES=\"0,1,7\" In these cases, the GPU orderings will appear as a consecutive list starting with 0. From inside an application, many software frameworks have ability to let you target specific GPUs, including tensorflow and pytorch: Tensorflow Pytorch","title":"GPU Selection"},{"location":"Singularity%20Containers/","text":"Nvidia Containers Nvidia delivers docker containers that contain their latest release of CUDA, tensorflow, pytorch, etc. You can see the full support matrix for all of their containers here: https://docs.nvidia.com/deeplearning/frameworks/support-matrix/index.html Docker is not runnable on ALCF's ThetaGPU system for most users, but singularity is. To convert one of these images to signularity you can use the following command: singularity build $OUTPUT_NAME $NVIDIA_CONTAINER_LOCATION where $OUTPUT_NAME is typically of the form tf2_20.09-py3.simg and $NVIDIA_CONTAINER_LOCATION can be a docker url such as docker://nvcr.io/nvidia/tensorflow:20.09-tf2-py3 You can find the latest containers from nvidia here: - Tensorflow 1 and 2: https://ngc.nvidia.com/catalog/containers/nvidia:tensorflow - Pytorch: https://ngc.nvidia.com/catalog/containers/nvidia:pytorch For your convienience, we've converted these containers to singularity and they are available for August, 2020 and September 2020 here: /lus/theta-fs0/software/thetagpu/nvidia-containers/ To extend the python libraries in these containers, please see https://github.com/argonne-lcf/ThetaGPU-Docs/blob/master/building_python_packages.md For running with these containers, please see [NEEDS LINK] For issues with these containers, please email support@alcf.anl.gov .","title":"Singularity Containers"},{"location":"Singularity%20Containers/#nvidia-containers","text":"Nvidia delivers docker containers that contain their latest release of CUDA, tensorflow, pytorch, etc. You can see the full support matrix for all of their containers here: https://docs.nvidia.com/deeplearning/frameworks/support-matrix/index.html Docker is not runnable on ALCF's ThetaGPU system for most users, but singularity is. To convert one of these images to signularity you can use the following command: singularity build $OUTPUT_NAME $NVIDIA_CONTAINER_LOCATION where $OUTPUT_NAME is typically of the form tf2_20.09-py3.simg and $NVIDIA_CONTAINER_LOCATION can be a docker url such as docker://nvcr.io/nvidia/tensorflow:20.09-tf2-py3 You can find the latest containers from nvidia here: - Tensorflow 1 and 2: https://ngc.nvidia.com/catalog/containers/nvidia:tensorflow - Pytorch: https://ngc.nvidia.com/catalog/containers/nvidia:pytorch For your convienience, we've converted these containers to singularity and they are available for August, 2020 and September 2020 here: /lus/theta-fs0/software/thetagpu/nvidia-containers/ To extend the python libraries in these containers, please see https://github.com/argonne-lcf/ThetaGPU-Docs/blob/master/building_python_packages.md For running with these containers, please see [NEEDS LINK] For issues with these containers, please email support@alcf.anl.gov .","title":"Nvidia Containers"},{"location":"building_python_packages/","text":"To build python packages for Theta GPU, there are two options: build on top of a bare-metal build (currently not available, but coming soon) or build on top of (and within) a singularity container. Additionally, you can build a new container from nvidia's docker images. Building on top of a container At the moment, you will need two shells to do this: have one open on a login node (for example, thetaloginN , and one open on a compute node ( thetagpuN ). First, start the container in interactive mode: singularity exec -B /lus:/lus --nv /lus/theta-fs0/projects/datascience/thetaGPU/containers/pytorch_20.08-py3.sif bash From here, you can create a virtual env for installation: export VENV_LOCATION=/path/to/virtualenv # replace this with your path! python -m venv --system-site-packages $VENV_LOCATION Note: sometimes, the venv package is available and if not, you can try python -m virtualenv . If neither are available, you can install it in your user directory: pip install --user virtualenv and it should work. Next time you log in, you'll have to start the container, and then run source $VENV_LOCATION/bin/activate to re-enable your installed packages. Reaching the outside world for pip packages You'll notice right away when you try to pip install you can not, because the connection fails. You can, however, go through a proxy server for pip by enabling these variables: export HTTP_PROXY=http://theta-proxy.tmi.alcf.anl.gov:3128 export HTTPS_PROXY=https://theta-proxy.tmi.alcf.anl.gov:3128 Now, you can pip install your favorite packages: pip install mpi4py Building custom packages Most packages (hdf5, for example, or python packages) can be built and installed into your virtual env. Here are two common examples that aren't currently part of the pytorch container that may be useful. HDF5 You can find the source code for hdf5 on their website https://www.hdfgroup.org/downloads/hdf5/source-code/. When downloaded and un-tarred, cd to the directory and run ./configure --prefix=$VENV_LOCATION # Add any other configuration arguments make -j 64 make install This should get you hdf5! For example, after this: (pytorch_20.08) Singularity> which h5cc /home/cadams/ThetaGPU/venvs/pytorch_20.08/bin/h5cc # This is my virtualenv, success! Horovod Horovod is useful for distributed training. To use it, you need it enabled within the container. git clone https://github.com/horovod/horovod.git cd horovod git submodule update --init python setup.py build python setup.py install This should install horovod within your container.","title":"Building Python Packages"},{"location":"building_python_packages/#building-on-top-of-a-container","text":"At the moment, you will need two shells to do this: have one open on a login node (for example, thetaloginN , and one open on a compute node ( thetagpuN ). First, start the container in interactive mode: singularity exec -B /lus:/lus --nv /lus/theta-fs0/projects/datascience/thetaGPU/containers/pytorch_20.08-py3.sif bash From here, you can create a virtual env for installation: export VENV_LOCATION=/path/to/virtualenv # replace this with your path! python -m venv --system-site-packages $VENV_LOCATION Note: sometimes, the venv package is available and if not, you can try python -m virtualenv . If neither are available, you can install it in your user directory: pip install --user virtualenv and it should work. Next time you log in, you'll have to start the container, and then run source $VENV_LOCATION/bin/activate to re-enable your installed packages.","title":"Building on top of a container"},{"location":"building_python_packages/#reaching-the-outside-world-for-pip-packages","text":"You'll notice right away when you try to pip install you can not, because the connection fails. You can, however, go through a proxy server for pip by enabling these variables: export HTTP_PROXY=http://theta-proxy.tmi.alcf.anl.gov:3128 export HTTPS_PROXY=https://theta-proxy.tmi.alcf.anl.gov:3128 Now, you can pip install your favorite packages: pip install mpi4py","title":"Reaching the outside world for pip packages"},{"location":"building_python_packages/#building-custom-packages","text":"Most packages (hdf5, for example, or python packages) can be built and installed into your virtual env. Here are two common examples that aren't currently part of the pytorch container that may be useful.","title":"Building custom packages"},{"location":"building_python_packages/#hdf5","text":"You can find the source code for hdf5 on their website https://www.hdfgroup.org/downloads/hdf5/source-code/. When downloaded and un-tarred, cd to the directory and run ./configure --prefix=$VENV_LOCATION # Add any other configuration arguments make -j 64 make install This should get you hdf5! For example, after this: (pytorch_20.08) Singularity> which h5cc /home/cadams/ThetaGPU/venvs/pytorch_20.08/bin/h5cc # This is my virtualenv, success!","title":"HDF5"},{"location":"building_python_packages/#horovod","text":"Horovod is useful for distributed training. To use it, you need it enabled within the container. git clone https://github.com/horovod/horovod.git cd horovod git submodule update --init python setup.py build python setup.py install This should install horovod within your container.","title":"Horovod"},{"location":"mpi/","text":"Launching a Singularity container with MPI #!/bin/bash SINGULARITYBIN=$(which singularity) CONTAINER=${HOME}/singularity_images/hpl-mofed5-cuda11runtime_verbs.sif OMPI_MCA_orte_launch_agent=$(cat <<EOF $SINGULARITYBIN run --nv $CONTAINER orted EOF ) export SINGULARITYENV_OMPI_MCA_orte_launch_agent=${OMPI_MCA_orte_launch_agent} $SINGULARITYBIN run --nv --cleanenv \\ $CONTAINER \\ mpirun \\ -H thetagpu01:1,thetagpu02:1 \\ -n 2 \\ --mca plm_rsh_args \"-p22\" \\ hostname","title":"MPI"},{"location":"mpi/#launching-a-singularity-container-with-mpi","text":"#!/bin/bash SINGULARITYBIN=$(which singularity) CONTAINER=${HOME}/singularity_images/hpl-mofed5-cuda11runtime_verbs.sif OMPI_MCA_orte_launch_agent=$(cat <<EOF $SINGULARITYBIN run --nv $CONTAINER orted EOF ) export SINGULARITYENV_OMPI_MCA_orte_launch_agent=${OMPI_MCA_orte_launch_agent} $SINGULARITYBIN run --nv --cleanenv \\ $CONTAINER \\ mpirun \\ -H thetagpu01:1,thetagpu02:1 \\ -n 2 \\ --mca plm_rsh_args \"-p22\" \\ hostname","title":"Launching a Singularity container with MPI"},{"location":"ml_frameworks/tensorflow/nvidia_container_notes/","text":"NVidia Container Notes Getting the container To get NVidia docker containers which have the latest CUDA and Tensorflow installed, go to NVidia NGC , create an account, search for Tensorflow . Notice there are containers tagged with tf1 and tf2 . The page tells you how to select the right one. You can convert the command at the top, for instance: docker pull nvcr.io/nvidia/tensorflow:20.08-tf2-py3 to a singularity command by doing this: singularity build tensorflow-20.08-tf2-py3.simg docker://nvcr.io/nvidia/tensorflow:20.08-tf2-py3 You'll need to run this command on a Theta login node which has network access ( thetaloginX ). The containers from August, 2020, are also all available converted to singularity here: /lus/theta-fs0/projects/datascience/thetaGPU/containers/ Running on ThetaGPU After logging into ThetaGPU with ssh thetagpusn1 , one can submit job using the container one a single node by doing: qsub -n 1 -t 10 -A <project-name> submit.sh where submit.sh contians the following bash scripting: #!/bin/bash CONTAINER=$HOME/tensorflow-20.08-tf2-py3.simg singularity exec --nv $CONTAINER python /usr/local/lib/python3.6/dist-packages/tensorflow/python/debug/examples/debug_mnist.py make sure to make the script executable with chmod a+x submit.sh . The log file <cobalt-jobid>.output should contain some text like this: Accuracy at step 0: 0.2159 Accuracy at step 1: 0.098 Accuracy at step 2: 0.098 Accuracy at step 3: 0.098 Accuracy at step 4: 0.098 Accuracy at step 5: 0.098 Accuracy at step 6: 0.098 Accuracy at step 7: 0.098 Accuracy at step 8: 0.098 Accuracy at step 9: 0.098 The numbers may be different. Running Tensorflow-2 with Horovod on ThetaGPU To run on ThetaGPU with MPI you can do the follow test: git clone git@github.com:jtchilders/tensorflow_skeleton.git cd tensorflow_skeleton qsub -n 2 -t 20 -A <project-name> submit_scripts/thetagpu_mnist.sh You can inspect the submit script for details on how the job is constructed.","title":"Container Notes"},{"location":"ml_frameworks/tensorflow/nvidia_container_notes/#nvidia-container-notes","text":"","title":"NVidia Container Notes"},{"location":"ml_frameworks/tensorflow/nvidia_container_notes/#getting-the-container","text":"To get NVidia docker containers which have the latest CUDA and Tensorflow installed, go to NVidia NGC , create an account, search for Tensorflow . Notice there are containers tagged with tf1 and tf2 . The page tells you how to select the right one. You can convert the command at the top, for instance: docker pull nvcr.io/nvidia/tensorflow:20.08-tf2-py3 to a singularity command by doing this: singularity build tensorflow-20.08-tf2-py3.simg docker://nvcr.io/nvidia/tensorflow:20.08-tf2-py3 You'll need to run this command on a Theta login node which has network access ( thetaloginX ). The containers from August, 2020, are also all available converted to singularity here: /lus/theta-fs0/projects/datascience/thetaGPU/containers/","title":"Getting the container"},{"location":"ml_frameworks/tensorflow/nvidia_container_notes/#running-on-thetagpu","text":"After logging into ThetaGPU with ssh thetagpusn1 , one can submit job using the container one a single node by doing: qsub -n 1 -t 10 -A <project-name> submit.sh where submit.sh contians the following bash scripting: #!/bin/bash CONTAINER=$HOME/tensorflow-20.08-tf2-py3.simg singularity exec --nv $CONTAINER python /usr/local/lib/python3.6/dist-packages/tensorflow/python/debug/examples/debug_mnist.py make sure to make the script executable with chmod a+x submit.sh . The log file <cobalt-jobid>.output should contain some text like this: Accuracy at step 0: 0.2159 Accuracy at step 1: 0.098 Accuracy at step 2: 0.098 Accuracy at step 3: 0.098 Accuracy at step 4: 0.098 Accuracy at step 5: 0.098 Accuracy at step 6: 0.098 Accuracy at step 7: 0.098 Accuracy at step 8: 0.098 Accuracy at step 9: 0.098 The numbers may be different.","title":"Running on ThetaGPU"},{"location":"ml_frameworks/tensorflow/nvidia_container_notes/#running-tensorflow-2-with-horovod-on-thetagpu","text":"To run on ThetaGPU with MPI you can do the follow test: git clone git@github.com:jtchilders/tensorflow_skeleton.git cd tensorflow_skeleton qsub -n 2 -t 20 -A <project-name> submit_scripts/thetagpu_mnist.sh You can inspect the submit script for details on how the job is constructed.","title":"Running Tensorflow-2 with Horovod on ThetaGPU"},{"location":"ml_frameworks/tensorflow/tensorboard_instructions/","text":"Tensorboard Instructions After you have logged into ThetaGPU, and have a Tensorflow run going, you'll need to know one of your worker nodes so you can SSH to it. PORT0=9991 PORT1=9992 PORT3=9993 # Select a theta login node N where N=[1-6] ssh -L $PORT0:localhost:$PORT1 $USER@thetaloginN.alcf.anl.gov # after reaching thetaloginN # Replace NN with your thetagpu worker node ssh -L $PORT1:thetagpuNN:$PORT3 $USER@thetagpusn1 # after reaching thetagpusn1 # login to worker node ssh thetagpuNN # now setup your tensorflow environment # for instance run the conda setup.sh script created during the install_tensorflow.sh script # now run tensorboard tensorboard --logdir </path/to/logs> --port $PORT3 --bind_all","title":"Tensorboard"},{"location":"ml_frameworks/tensorflow/tensorboard_instructions/#tensorboard-instructions","text":"After you have logged into ThetaGPU, and have a Tensorflow run going, you'll need to know one of your worker nodes so you can SSH to it. PORT0=9991 PORT1=9992 PORT3=9993 # Select a theta login node N where N=[1-6] ssh -L $PORT0:localhost:$PORT1 $USER@thetaloginN.alcf.anl.gov # after reaching thetaloginN # Replace NN with your thetagpu worker node ssh -L $PORT1:thetagpuNN:$PORT3 $USER@thetagpusn1 # after reaching thetagpusn1 # login to worker node ssh thetagpuNN # now setup your tensorflow environment # for instance run the conda setup.sh script created during the install_tensorflow.sh script # now run tensorboard tensorboard --logdir </path/to/logs> --port $PORT3 --bind_all","title":"Tensorboard Instructions"}]}